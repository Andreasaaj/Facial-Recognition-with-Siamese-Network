{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeba_train = torchvision.datasets.CelebA(root=\"data/\", split=\"train\", target_type='identity', download=True, transform=torchvision.transforms.ToTensor())\n",
    "celeba_valid = torchvision.datasets.CelebA(root=\"data/\", split=\"valid\", target_type='identity', download=True, transform=torchvision.transforms.ToTensor())\n",
    "# celeba_test = torchvision.datasets.CelebA(root=\"data/\", split=\"test\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom sampler class that generates batches with of triplets of images\n",
    "class TripletSampler(torch.utils.data.sampler.Sampler):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Get a random anchor image and its identity\n",
    "        _, anchor_identity = self.dataset[i]\n",
    "        # Get all indexes with the same identity\n",
    "        same_identity_idx = torch.where(self.dataset.identity == anchor_identity)[0]\n",
    "        # remove the anchor image from the list\n",
    "        same_identity_idx = same_identity_idx[same_identity_idx != i]\n",
    "        # Get a random positive image\n",
    "        positive_idx = same_identity_idx[torch.randint(len(same_identity_idx), (1,))]\n",
    "        # Get all indexes with different identities\n",
    "        different_identity_idx = torch.where(self.dataset.identity != anchor_identity)[0]\n",
    "        # Get a random negative image\n",
    "        negative_idx = different_identity_idx[torch.randint(len(different_identity_idx), (1,))]\n",
    "        return (i, positive_idx.item(), negative_idx.item())\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(len(self.dataset)):\n",
    "            yield self[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "class TripletDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, triplet_idx):\n",
    "        anchor_idx, positive_idx, negative_idx = triplet_idx\n",
    "        a_img, a_label = self.dataset[anchor_idx]\n",
    "        p_img, p_label = self.dataset[positive_idx]\n",
    "        n_img, n_label = self.dataset[negative_idx]\n",
    "        assert a_label == p_label and a_label != n_label\n",
    "        return (a_img, p_img, n_img), (a_label.item(), p_label.item(), n_label.item())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "triplet_sampler = TripletSampler(celeba_train)\n",
    "triplet_dataset = TripletDataset(celeba_train)\n",
    "triplet_data_loader = torch.utils.data.DataLoader(  triplet_dataset,\n",
    "                                                    batch_size=16,\n",
    "                                                    #shuffle=False, # Should be True later\n",
    "                                                    num_workers=0, # Needs to be 0 for me or calling iter never returns\n",
    "                                                    sampler=triplet_sampler,\n",
    "                                                  )\n",
    "triplet_iter = iter(triplet_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_triplet(imgs, labels):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(10, 10))\n",
    "    titles = [\"Anchor\", \"Positive\", \"Negative\"]\n",
    "    for i, img in enumerate(imgs):\n",
    "        ax[i].imshow(img.permute(1, 2, 0))\n",
    "        # ax[i].set_title(f\"Identity: {labels[i]}\")\n",
    "        ax[i].set_title(titles[i] + f\"\\nIdentity: {labels[i]}\")\n",
    "        ax[i].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_iter = iter(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A triplet batch is (Labels, Images) x (Anchor, Positive, Negative) x Batch Size x Channels x Height x Width\n",
    "# = 2 x 3 x 16 x 3 x 218 x 178\n",
    "\n",
    "def example():\n",
    "    # triplet_batch = next(triplet_iter)\n",
    "    # imgs = [triplet_batch[0][0][0], triplet_batch[0][1][0], triplet_batch[0][2][0]]\n",
    "    # labels = [triplet_batch[1][0][0], triplet_batch[1][1][0], triplet_batch[1][2][0]]\n",
    "    # plot_triplet(imgs, labels)\n",
    "\n",
    "    idx = triplet_sampler[next(example_iter)]\n",
    "    triplet_batch = triplet_dataset.__getitem__(idx)\n",
    "    print(\"Triplet Batch size:\")\n",
    "    print(len(triplet_batch), len(triplet_batch[0]), \"16\", len(triplet_batch[0][0]), *triplet_batch[0][0][0].shape, sep=' x ')\n",
    "    plot_triplet(triplet_batch[0], triplet_batch[1])\n",
    "    \n",
    "example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(self).__init__() # Why Tripletnet? \n",
    "        self.resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.resnet.fc = nn.Identity() # Override last layer with no impl \n",
    "        self.fc = nn.Linear(512, 1000) # Input features is 512 and output layers correspond to number of classes in ResNet, i.e. 1000 \n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        anchor = self.resnet(anchor)\n",
    "        anchor = self.fc(anchor)\n",
    "        positive = self.resnet(positive)\n",
    "        positive = self.fc(positive)\n",
    "        negative = self.resnet(negative)\n",
    "        negative = self.fc(negative)\n",
    "        return anchor, positive, negative\n",
    "\n",
    "\n",
    "def train_triplet_net(triplet_net, triplet_data_loader, epochs=1, lr=0.001, freeze=False):\n",
    "    if freeze:\n",
    "        # freeze all layers except the last fc layer\n",
    "        for param in triplet_net.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        triplet_net.fc.requires_grad = True\n",
    "    \n",
    "    triplet_net.train()\n",
    "    optimizer = optim.Adam(triplet_net.parameters(), lr=lr)\n",
    "    criterion = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, triplet_batch in enumerate(triplet_data_loader):\n",
    "            optimizer.zero_grad()\n",
    "            anchor, positive, negative = triplet_batch[0]\n",
    "            anchor, positive, negative = anchor.cuda(), positive.cuda(), negative.cuda()\n",
    "            anchor, positive, negative = triplet_net(anchor, positive, negative)\n",
    "            loss = criterion(anchor, positive, negative)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print(f\"Epoch: {epoch + 1}, Batch: {i + 1}, Loss: {running_loss / 100}\")\n",
    "                running_loss = 0.0\n",
    "            if i == 100:\n",
    "                break\n",
    "    print(\"Finished Training\")\n",
    "\n",
    "\n",
    "def test_triplet_net(triplet_net, triplet_data_loader):\n",
    "    triplet_net.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, triplet_batch in enumerate(triplet_data_loader):\n",
    "            anchor, positive, negative = triplet_batch[0]\n",
    "            anchor, positive, negative = anchor.cuda(), positive.cuda(), negative.cuda()\n",
    "            anchor, positive, negative = triplet_net(anchor, positive, negative)\n",
    "\n",
    "\n",
    "triplet_net = TripletNet()\n",
    "triplet_net.cuda()\n",
    "train_triplet_net(triplet_net, triplet_data_loader, epochs=3, lr=0.001, freeze=True)\n",
    "train_triplet_net(triplet_net, triplet_data_loader, epochs=3, lr=0.001, freeze=False)\n",
    "# test_triplet_net(triplet_net, triplet_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(triplet_net.state_dict(), \"models/triplet_net.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_net = TripletNet()\n",
    "triplet_net.load_state_dict(torch.load(\"models/triplet_net.pt\"))\n",
    "triplet_net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the embeddings of the all images of the first 10 people\n",
    "\n",
    "# get all images of the first 10 people\n",
    "first_10_idx = torch.where(celeba_train.identity <= 10)[0]\n",
    "\n",
    "# get all embeddings of triplet_net of the first 10 people\n",
    "first_10_embeddings = []\n",
    "# first_10_imgs = []\n",
    "for i in first_10_idx:\n",
    "    img, label = celeba_train[i]\n",
    "    # first_10_imgs.append(img)\n",
    "    img = img.cuda()\n",
    "    img = img.unsqueeze(0)\n",
    "    embedding = triplet_net.resnet(img)\n",
    "    first_10_embeddings.append(embedding)\n",
    "\n",
    "first_10_labels = celeba_train.identity[first_10_idx].numpy()\n",
    "\n",
    "# find first image of each of the first 10 people\n",
    "first_10_imgs = []\n",
    "for i in range(1,11):\n",
    "    idx = torch.where(celeba_train.identity == i)[0][0]\n",
    "    img, label = celeba_train[idx]\n",
    "    first_10_imgs.append(img)\n",
    "\n",
    "# show one image of each of the first 10 people\n",
    "fig, ax = plt.subplots(1, 10, figsize=(20, 20))\n",
    "for i, img in enumerate(first_10_imgs):\n",
    "    ax[i].imshow(img.permute(1, 2, 0))\n",
    "    ax[i].set_title(f\"Identity: {i + 1}\")\n",
    "    ax[i].axis('off')\n",
    "plt.show()\n",
    "\n",
    "# plot the embeddings of the first 10 people\n",
    "first_10_embeddings = torch.stack(first_10_embeddings)\n",
    "first_10_embeddings = first_10_embeddings.squeeze(1)\n",
    "first_10_embeddings = first_10_embeddings.cpu().detach().numpy()\n",
    "plt.scatter(first_10_embeddings[:, 0], first_10_embeddings[:, 1], c=first_10_labels)\n",
    "plt.show()\n",
    "\n",
    "# print identity with lowest and largest embedding value\n",
    "max_idx = np.argmax(first_10_embeddings[:, 0])\n",
    "print(\"Identity with largest embedding value:\")\n",
    "print(first_10_labels[max_idx])\n",
    "min_idx = np.argmin(first_10_embeddings[:, 0])\n",
    "print(\"Identity with lowest embedding value:\")\n",
    "print(first_10_labels[min_idx])\n",
    "\n",
    "# Show the embeddings of the first 10 people with TSNE\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "first_10_embeddings_tsne = tsne.fit_transform(first_10_embeddings)\n",
    "# show legend with 10 identities\n",
    "plt.legend([f\"Identity: {i + 1}\" for i in range(10)])\n",
    "plt.scatter(first_10_embeddings_tsne[:, 0], first_10_embeddings_tsne[:, 1], c=first_10_labels, cmap='tab10')\n",
    "plt.show()\n",
    "\n",
    "# print identity with lowest and largest TNES value\n",
    "max_idx = np.argmax(first_10_embeddings_tsne[:, 0])\n",
    "print(\"Identity with largest TNES value:\")\n",
    "print(first_10_labels[max_idx])\n",
    "min_idx = np.argmin(first_10_embeddings_tsne[:, 0])\n",
    "print(\"Identity with lowest TNES value:\")\n",
    "print(first_10_labels[min_idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Training function (fit) (PROVIDED BY HENRIK)\n",
    "The code below can be used to train a model and monitor important stats as training progresses. The training is carried out by calling the `fit` function, which takes any model as input.\n",
    "\n",
    "You can change the optimizer by replacing `base_optimizer` with your own function handle.\n",
    "\n",
    "The function `base_lr_scheduler` is a learning rate scheduler that updates the learning rate of the optimizer during training. The dummy-implementation does nothing, but you can modify it to implement your own learning rate scheduler. The inputs are\n",
    "- `T` : Total number of batches\n",
    "- `t` : Current batch index (max(t) = T)\n",
    "- `lr` : Current learning rate\n",
    "\n",
    "Other parameters to `fit` are:\n",
    "- `bs` the batch size\n",
    "- `epochs` the number of epochs\n",
    "- `batches_per_epoch` the number of batches per epoch. If set to `None`, all images in the dataset are used.\n",
    "\n",
    "Note that you can use `batches_per_epoch` to reduce the size of the training set. The effective size of the training set is `bs*batches_per_epoch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Function handle that returns an optimizer\n",
    "def base_optimizer(model,lr=0.001, momentum=0.9):\n",
    "    return optim.SGD(model.parameters(), lr=lr,momentum=momentum)\n",
    "\n",
    "# Function handle that updates the learning rate\n",
    "# (note this is a dummy implementation that does nothing)\n",
    "def base_lr_scheduler(t,T,lr):\n",
    "  return lr\n",
    "\n",
    "# Function to fit a model\n",
    "def fit(model,\n",
    "        opt_func=base_optimizer, # Inject own optimizer here\n",
    "        lr_scheduler=base_lr_scheduler, # Inject own scheduler here\n",
    "        bs=256,\n",
    "        epochs=1,\n",
    "        batches_per_epoch=None, # Default: Use entire training set\n",
    "        show_summary=True):\n",
    "\n",
    "  # Set up data loaders\n",
    "  if batches_per_epoch == None:\n",
    "    # Use all images\n",
    "    train_dl = torch.utils.data.DataLoader(trainset, batch_size=bs,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "    valid_dl = torch.utils.data.DataLoader(testset, batch_size=bs,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "    batches_per_epoch = len(train_dl)\n",
    "  else:\n",
    "    # Only use a subset of the data\n",
    "    subset_indices = list(range(batches_per_epoch*bs))\n",
    "    train_dl = torch.utils.data.DataLoader(trainset, batch_size=bs, sampler=torch.utils.data.sampler.SubsetRandomSampler(subset_indices), num_workers=2)\n",
    "\n",
    "    # Use one fourth for validation\n",
    "    subset_indices = list(range(int(np.ceil(batches_per_epoch/4))*bs))\n",
    "    valid_dl = torch.utils.data.DataLoader(testset, batch_size=bs, sampler=torch.utils.data.sampler.SubsetRandomSampler(subset_indices), num_workers=2)\n",
    "\n",
    "  # Initialize optimizer\n",
    "  opt = opt_func(model)\n",
    "\n",
    "  # For book keeping\n",
    "  train_loss_history = []\n",
    "  valid_loss_history = []\n",
    "  plot_time_train = []\n",
    "  plot_time_valid = []\n",
    "\n",
    "  # Index of current batch\n",
    "  t = 1\n",
    "\n",
    "  # Total number of batches\n",
    "  T = batches_per_epoch * epochs\n",
    "\n",
    "  print('Epochs:',epochs,'Batches per epoch:',batches_per_epoch,'Total number of batches',T)\n",
    "\n",
    "  # Get initial validation loss and accuracy\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    valid_acc = sum(accuracy(model(xb.cuda()), yb.cuda()) for xb, yb in valid_dl) / len(valid_dl)\n",
    "    valid_loss = sum(loss_func(model(xb.cuda()), yb.cuda()) for xb, yb in valid_dl) / len(valid_dl)\n",
    "    valid_loss_history.append(valid_loss.detach().cpu().numpy())\n",
    "    plot_time_valid.append(t)\n",
    "\n",
    "  # Train\n",
    "  for epoch in range(epochs):\n",
    "    model.train() # Train mode\n",
    "    for xb, yb in train_dl:\n",
    "\n",
    "      # Update learning rate\n",
    "      opt.param_groups[0]['lr'] = lr_scheduler(t,T,lr=opt.param_groups[0]['lr'])\n",
    "\n",
    "      # Forward prop\n",
    "      pred = model(xb.cuda())\n",
    "      loss = loss_func(pred, yb.cuda())\n",
    "\n",
    "      # Book keeping\n",
    "      train_loss_history.append(loss.detach().cpu().numpy())\n",
    "      plot_time_train.append(t)\n",
    "      t += 1\n",
    "\n",
    "      # Backward prop (calculate gradient)\n",
    "      loss.backward()\n",
    "\n",
    "      # Update model parameters\n",
    "      opt.step()\n",
    "      opt.zero_grad()\n",
    "\n",
    "      # Validation loss and accuracy\n",
    "      if t % 10 == 0:    # print every 10 mini-batches\n",
    "        model.eval() # Test mode\n",
    "        with torch.no_grad():\n",
    "            valid_acc = sum(accuracy(model(xb.cuda()), yb.cuda()) for xb, yb in valid_dl) / len(valid_dl)\n",
    "            valid_loss = sum(loss_func(model(xb.cuda()), yb.cuda()) for xb, yb in valid_dl) / len(valid_dl)\n",
    "            valid_loss_history.append(valid_loss.detach().cpu().numpy())\n",
    "            plot_time_valid.append(t-1)\n",
    "            print('t',t,'lr',opt.param_groups[0]['lr'],'train loss',loss.detach().cpu().numpy(), 'val loss',valid_loss.detach().cpu().numpy(),'val accuracy', valid_acc.detach().cpu().numpy())\n",
    "        model.train() # Back to train mode\n",
    "\n",
    "  # Summary\n",
    "  if show_summary:\n",
    "    plt.figure()\n",
    "    lines = []\n",
    "    labels = []\n",
    "    l, = plt.plot(plot_time_train,train_loss_history)\n",
    "    lines.append(l)\n",
    "    labels.append('Training')\n",
    "    print(valid_loss_history)\n",
    "    l, = plt.plot(plot_time_valid,valid_loss_history)\n",
    "    lines.append(l)\n",
    "    labels.append('Validation')\n",
    "    plt.title('Loss')\n",
    "    plt.legend(lines, labels, loc=(1, 0), prop=dict(size=14))\n",
    "    plt.show()\n",
    "\n",
    "  return train_loss_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic function to train a model\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    # Copy weights\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history only if in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic function to display predictions for a few images\n",
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(2, num_images//2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('pred/true: {}/{}'.format(class_names[preds[j]],\n",
    "                                                       class_names[labels[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
